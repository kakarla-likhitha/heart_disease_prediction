# -*- coding: utf-8 -*-
"""heart_disease_predicion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16aaLvUL0PZviuIoQgjnjO4tvE4pZFE7K

importing libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
# %matplotlib inline
from matplotlib.colors import ListedColormap
from sklearn.model_selection import GridSearchCV, StratifiedKFold

"""Reading dataset"""

df=pd.read_csv('heart.csv')
df

df.info()

#converting categorical data into string(object) data type
continous_features=['age','trestbps','chol','thalach','oldpeak']
features_to_convert=[feature for feature in df.columns if feature not in continous_features]
df[features_to_convert]=df[features_to_convert].astype('object')
df.dtypes

df.describe()

#summary statistics for categorical variables
df.describe(include='object')

df.isnull().sum()

continous_features

"""**finding the outliers**"""

Q1=df[continous_features].quantile(0.25)
Q3=df[continous_features].quantile(0.75)
IQR=Q3-Q1
outliers_count=((df[continous_features]<(Q1-1.5*IQR))|(df[continous_features]>(Q3+1.5*IQR))).sum()
outliers_count

df_encoded=pd.get_dummies(df,columns=['cp','restecg','thal'],drop_first=True).astype(int)

df_encoded

features_to_convert=['sex', 'fbs', 'exang', 'slope', 'ca', 'target']
for feature in features_to_convert:
  df_encoded[feature]=df_encoded[feature].astype(int)
df_encoded.dtypes

df_encoded.head()

x=df_encoded.drop('target',axis=1)
y=df_encoded['target']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0,stratify=y)

continous_features

x_train['oldpeak']=x_train['oldpeak']+0.001
x_test['oldpeak']=x_test['oldpeak']+0.001

"""**Decision Tree Model** **Building**"""

dt_base=DecisionTreeClassifier(random_state=0)

"""Hyperparameter tuning"""

def tune_hyperparameters(clf,param_grid,x_train,y_train,scoring='recall',n_splits=3):
  cv=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=0)
  clf_grid=GridSearchCV(clf,param_grid,cv=cv,scoring=scoring,n_jobs=-1)
  clf_grid.fit(x_train,y_train)
  best_hyperparameters=clf_grid.best_params_
  return clf_grid.best_estimator_,best_hyperparameters

param_grid_dt={
    'criterion': ['gini', 'entropy'],
    'max_depth': [2,3],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2]
}

best_dt, best_dt_hyperparams = tune_hyperparameters(dt_base, param_grid_dt, x_train, y_train)

print(best_dt_hyperparams)

#evalution of model on training data

print(classification_report(y_train,best_dt.predict(x_train)))

print(classification_report(y_test,best_dt.predict(x_test)))

"""**creating a function that consolidates each model's metrics into a dataframe**"""

def evaluate_model(model,x_test,y_test,model_name):
  y_pred=model.predict(x_test)
  report=classification_report(y_test,y_pred,output_dict=True)
  metrics={
        "precision_0": report["0"]["precision"],
        "precision_1": report["1"]["precision"],
        "recall_0": report["0"]["recall"],
        "recall_1": report["1"]["recall"],
        "f1_0": report["0"]["f1-score"],
        "f1_1": report["1"]["f1-score"],
        "macro_avg_precision": report["macro avg"]["precision"],
        "macro_avg_recall": report["macro avg"]["recall"],
        "macro_avg_f1": report["macro avg"]["f1-score"],
        "accuracy": accuracy_score(y_test, y_pred)
    }
  df=pd.DataFrame(metrics,index=[model_name]).round(2)
  return df

dt_evaluation=evaluate_model(best_dt,x_test,y_test,'DT')
dt_evaluation

"""RANDOM FOREST MODEL"""

rf_base=RandomForestClassifier(random_state=0)

param_grid_rf={
    'n_estimators':[10,30,50,70,100],
    'criterion':['gini','entropy'],
    'max_depth':[2,3,4],
    'min_samples_split':[2,3,4,5],
    'min_samples_leaf':[1,2,3],
    'bootstrap':[True,False]
}

best_rf,best_rf_hyperparams=tune_hyperparameters(rf_base,param_grid_rf,x_train,y_train)
print("best params",best_rf_hyperparams)

print(classification_report(y_train,best_rf.predict(x_train)))

print(classification_report(y_test,best_rf.predict(x_test)))

rf_evaluation=evaluate_model(best_rf,x_test,y_test,'RF')
rf_evaluation

"""**KNN**"""

from sklearn.pipeline import Pipeline
knn_pipeline=Pipeline([
    ('scaler',StandardScaler()),
    ('knn',KNeighborsClassifier())
])

knn_param_grid={
    'knn__n_neighbors':list(range(1,12)),
    'knn__weights':['uniform','distance'],
    'knn__p':[1,2]
}

best_knn,best_knn_hyperparams=tune_hyperparameters(knn_pipeline,knn_param_grid,x_train,y_train)
print("best params knn",best_knn_hyperparams)

print(classification_report(y_train,best_knn.predict(x_train)))

print(classification_report(y_test,best_knn.predict(x_test)))

knn_evaluation=evaluate_model(best_knn,x_test,y_test,'KNN')
knn_evaluation

"""**SVM**"""

svm_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('svm', SVC())
])

param_grid_svm={
    'svm__C':[0.0011, 0.005, 0.01, 0.05, 0.1, 1, 10, 20],
    'svm__kernel':['linear'],
    'svm__gamma':['scale','auto',0.1,0.5,1,5],
    'svm__degree':[2,3,4]
}

best_svm,best_svm_hyperparams=tune_hyperparameters(svm_pipeline,param_grid_svm,x_train,y_train)
print("best params",best_svm_hyperparams)

print(classification_report(y_train,best_svm.predict(x_train)))

print(classification_report(y_test,best_svm.predict(x_test)))

svm_evaluation=evaluate_model(best_svm,x_test,y_test,'SVM')
svm_evaluation

all_evaluations=[dt_evaluation,rf_evaluation,svm_evaluation,knn_evaluation]
results=pd.concat(all_evaluations)
results.sort_values(by='recall_1',ascending=False)